{
 "cells": [
  {
   "cell_type": "code",
   "id": "51764ead",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:01.004886Z",
     "start_time": "2025-11-08T13:45:00.744383Z"
    }
   },
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "from torch import nn\n",
    "# from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "logs_dir = \"tensorboard\"\n",
    "!pkill -f tensorboard\n",
    "%load_ext tensorboard\n",
    "!mkdir -p models\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Import other libraries\n",
    "import copy\n",
    "import shutil\n",
    "from itertools import product\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "PyTorch version: 2.9.0+cu128\n",
      "Device: cuda\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "id": "a5320974",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:01.341277Z",
     "start_time": "2025-11-08T13:45:01.009932Z"
    }
   },
   "source": [
    "# Read the dataset into a DataFrame with specified column names\n",
    "df = pd.read_csv('pirate_pain_train.csv', header=0)\n",
    "\n",
    "df_labels = pd.read_csv('pirate_pain_train_labels.csv', header=0)\n",
    "\n",
    "# Print the shape of the DataFrame\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "\n",
    "# Display the first 10 rows of the DataFrame\n",
    "df.head(10)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (105760, 40)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
       "0             0     0              2              0              2   \n",
       "1             0     1              2              2              2   \n",
       "2             0     2              2              0              2   \n",
       "3             0     3              2              2              2   \n",
       "4             0     4              2              2              2   \n",
       "5             0     5              2              0              2   \n",
       "6             0     6              2              1              2   \n",
       "7             0     7              2              2              2   \n",
       "8             0     8              2              2              0   \n",
       "9             0     9              0              2              2   \n",
       "\n",
       "   pain_survey_4 n_legs n_hands n_eyes  joint_00  ...      joint_21  \\\n",
       "0              1    two     two    two  1.094705  ...  3.499558e-06   \n",
       "1              2    two     two    two  1.135183  ...  3.976952e-07   \n",
       "2              2    two     two    two  1.080745  ...  1.533820e-07   \n",
       "3              2    two     two    two  0.938017  ...  1.006865e-05   \n",
       "4              2    two     two    two  1.090185  ...  4.437266e-06   \n",
       "5              1    two     two    two  1.146031  ...  1.073167e-06   \n",
       "6              1    two     two    two  1.025870  ...  1.074800e-06   \n",
       "7              2    two     two    two  1.038597  ...  8.829074e-07   \n",
       "8              1    two     two    two  0.984251  ...  1.621055e-06   \n",
       "9              2    two     two    two  1.054999  ...  1.609114e-06   \n",
       "\n",
       "       joint_22      joint_23      joint_24  joint_25  joint_26  joint_27  \\\n",
       "0  1.945042e-06  3.999558e-06  1.153299e-05  0.000004  0.017592  0.013508   \n",
       "1  6.765107e-07  6.019627e-06  4.643774e-08  0.000000  0.013352  0.000000   \n",
       "2  1.698525e-07  1.446051e-06  2.424536e-06  0.000003  0.016225  0.008110   \n",
       "3  5.511079e-07  1.847597e-06  5.432416e-08  0.000000  0.011832  0.007450   \n",
       "4  1.735459e-07  1.552722e-06  5.825366e-08  0.000007  0.005360  0.002532   \n",
       "5  1.753837e-07  2.957340e-07  6.217311e-08  0.000007  0.006150  0.006444   \n",
       "6  1.772156e-07  1.976558e-06  1.576086e-06  0.000005  0.006495  0.006421   \n",
       "7  1.790415e-07  2.210562e-06  1.485741e-06  0.000000  0.015998  0.005397   \n",
       "8  1.165161e-06  3.030164e-07  5.416678e-07  0.000000  0.020539  0.008517   \n",
       "9  3.959558e-06  2.017157e-06  1.154349e-06  0.000007  0.007682  0.021383   \n",
       "\n",
       "   joint_28  joint_29  joint_30  \n",
       "0  0.026798  0.027815       0.5  \n",
       "1  0.013377  0.013716       0.5  \n",
       "2  0.024097  0.023105       0.5  \n",
       "3  0.028613  0.024648       0.5  \n",
       "4  0.033026  0.025328       0.5  \n",
       "5  0.033101  0.023767       0.5  \n",
       "6  0.031804  0.019056       0.5  \n",
       "7  0.035552  0.015732       0.5  \n",
       "8  0.008635  0.015257       0.5  \n",
       "9  0.034006  0.028966       0.5  \n",
       "\n",
       "[10 rows x 40 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>n_legs</th>\n",
       "      <th>n_hands</th>\n",
       "      <th>n_eyes</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_21</th>\n",
       "      <th>joint_22</th>\n",
       "      <th>joint_23</th>\n",
       "      <th>joint_24</th>\n",
       "      <th>joint_25</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "      <th>joint_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.094705</td>\n",
       "      <td>...</td>\n",
       "      <td>3.499558e-06</td>\n",
       "      <td>1.945042e-06</td>\n",
       "      <td>3.999558e-06</td>\n",
       "      <td>1.153299e-05</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.017592</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>0.026798</td>\n",
       "      <td>0.027815</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.135183</td>\n",
       "      <td>...</td>\n",
       "      <td>3.976952e-07</td>\n",
       "      <td>6.765107e-07</td>\n",
       "      <td>6.019627e-06</td>\n",
       "      <td>4.643774e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013377</td>\n",
       "      <td>0.013716</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.080745</td>\n",
       "      <td>...</td>\n",
       "      <td>1.533820e-07</td>\n",
       "      <td>1.698525e-07</td>\n",
       "      <td>1.446051e-06</td>\n",
       "      <td>2.424536e-06</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.016225</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.024097</td>\n",
       "      <td>0.023105</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>0.938017</td>\n",
       "      <td>...</td>\n",
       "      <td>1.006865e-05</td>\n",
       "      <td>5.511079e-07</td>\n",
       "      <td>1.847597e-06</td>\n",
       "      <td>5.432416e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011832</td>\n",
       "      <td>0.007450</td>\n",
       "      <td>0.028613</td>\n",
       "      <td>0.024648</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.090185</td>\n",
       "      <td>...</td>\n",
       "      <td>4.437266e-06</td>\n",
       "      <td>1.735459e-07</td>\n",
       "      <td>1.552722e-06</td>\n",
       "      <td>5.825366e-08</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.005360</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.033026</td>\n",
       "      <td>0.025328</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.146031</td>\n",
       "      <td>...</td>\n",
       "      <td>1.073167e-06</td>\n",
       "      <td>1.753837e-07</td>\n",
       "      <td>2.957340e-07</td>\n",
       "      <td>6.217311e-08</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.006444</td>\n",
       "      <td>0.033101</td>\n",
       "      <td>0.023767</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.025870</td>\n",
       "      <td>...</td>\n",
       "      <td>1.074800e-06</td>\n",
       "      <td>1.772156e-07</td>\n",
       "      <td>1.976558e-06</td>\n",
       "      <td>1.576086e-06</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.006495</td>\n",
       "      <td>0.006421</td>\n",
       "      <td>0.031804</td>\n",
       "      <td>0.019056</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.038597</td>\n",
       "      <td>...</td>\n",
       "      <td>8.829074e-07</td>\n",
       "      <td>1.790415e-07</td>\n",
       "      <td>2.210562e-06</td>\n",
       "      <td>1.485741e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015998</td>\n",
       "      <td>0.005397</td>\n",
       "      <td>0.035552</td>\n",
       "      <td>0.015732</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>0.984251</td>\n",
       "      <td>...</td>\n",
       "      <td>1.621055e-06</td>\n",
       "      <td>1.165161e-06</td>\n",
       "      <td>3.030164e-07</td>\n",
       "      <td>5.416678e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020539</td>\n",
       "      <td>0.008517</td>\n",
       "      <td>0.008635</td>\n",
       "      <td>0.015257</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.054999</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609114e-06</td>\n",
       "      <td>3.959558e-06</td>\n",
       "      <td>2.017157e-06</td>\n",
       "      <td>1.154349e-06</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>0.021383</td>\n",
       "      <td>0.034006</td>\n",
       "      <td>0.028966</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 40 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "id": "b9c06179",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:01.560366Z",
     "start_time": "2025-11-08T13:45:01.541928Z"
    }
   },
   "source": [
    "# NOTE: These could be removed OR REMOVE THE DATAPOINTS WITH THESE DIFFERENT FROM 2\n",
    "print(df[\"n_legs\"].value_counts())\n",
    "print(df[\"n_hands\"].value_counts())\n",
    "print(df[\"n_eyes\"].value_counts())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_legs\n",
      "two            104800\n",
      "one+peg_leg       960\n",
      "Name: count, dtype: int64\n",
      "n_hands\n",
      "two              104800\n",
      "one+hook_hand       960\n",
      "Name: count, dtype: int64\n",
      "n_eyes\n",
      "two              104800\n",
      "one+eye_patch       960\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:01.688433Z",
     "start_time": "2025-11-08T13:45:01.664974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for col in df.columns[6:9]:\n",
    "    df[col] = np.where(df[col] == 'two', 2, 1)\n",
    "float_cols = df.select_dtypes(include=['float64', 'int64']).columns[2:]\n",
    "for col in float_cols:\n",
    "    df[col] = df[col].astype(np.float32)"
   ],
   "id": "0cc6fb96",
   "outputs": [],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "id": "a93d4ace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:01.727095Z",
     "start_time": "2025-11-08T13:45:01.719341Z"
    }
   },
   "source": [
    "df.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105760 entries, 0 to 105759\n",
      "Data columns (total 40 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   sample_index   105760 non-null  int64  \n",
      " 1   time           105760 non-null  int64  \n",
      " 2   pain_survey_1  105760 non-null  float32\n",
      " 3   pain_survey_2  105760 non-null  float32\n",
      " 4   pain_survey_3  105760 non-null  float32\n",
      " 5   pain_survey_4  105760 non-null  float32\n",
      " 6   n_legs         105760 non-null  float32\n",
      " 7   n_hands        105760 non-null  float32\n",
      " 8   n_eyes         105760 non-null  float32\n",
      " 9   joint_00       105760 non-null  float32\n",
      " 10  joint_01       105760 non-null  float32\n",
      " 11  joint_02       105760 non-null  float32\n",
      " 12  joint_03       105760 non-null  float32\n",
      " 13  joint_04       105760 non-null  float32\n",
      " 14  joint_05       105760 non-null  float32\n",
      " 15  joint_06       105760 non-null  float32\n",
      " 16  joint_07       105760 non-null  float32\n",
      " 17  joint_08       105760 non-null  float32\n",
      " 18  joint_09       105760 non-null  float32\n",
      " 19  joint_10       105760 non-null  float32\n",
      " 20  joint_11       105760 non-null  float32\n",
      " 21  joint_12       105760 non-null  float32\n",
      " 22  joint_13       105760 non-null  float32\n",
      " 23  joint_14       105760 non-null  float32\n",
      " 24  joint_15       105760 non-null  float32\n",
      " 25  joint_16       105760 non-null  float32\n",
      " 26  joint_17       105760 non-null  float32\n",
      " 27  joint_18       105760 non-null  float32\n",
      " 28  joint_19       105760 non-null  float32\n",
      " 29  joint_20       105760 non-null  float32\n",
      " 30  joint_21       105760 non-null  float32\n",
      " 31  joint_22       105760 non-null  float32\n",
      " 32  joint_23       105760 non-null  float32\n",
      " 33  joint_24       105760 non-null  float32\n",
      " 34  joint_25       105760 non-null  float32\n",
      " 35  joint_26       105760 non-null  float32\n",
      " 36  joint_27       105760 non-null  float32\n",
      " 37  joint_28       105760 non-null  float32\n",
      " 38  joint_29       105760 non-null  float32\n",
      " 39  joint_30       105760 non-null  float32\n",
      "dtypes: float32(38), int64(2)\n",
      "memory usage: 16.9 MB\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "id": "b99ed791",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:01.767528Z",
     "start_time": "2025-11-08T13:45:01.765825Z"
    }
   },
   "source": "#df.drop(columns=[\"n_legs\", \"n_hands\", \"n_eyes\"], inplace=True)",
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "id": "f1712ef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:01.824559Z",
     "start_time": "2025-11-08T13:45:01.818355Z"
    }
   },
   "source": [
    "df.head(5)\n",
    "print(df[\"n_legs\"].value_counts())\n",
    "print(df[\"n_hands\"].value_counts())\n",
    "print(df[\"n_eyes\"].value_counts())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_legs\n",
      "2.0    104800\n",
      "1.0       960\n",
      "Name: count, dtype: int64\n",
      "n_hands\n",
      "2.0    104800\n",
      "1.0       960\n",
      "Name: count, dtype: int64\n",
      "n_eyes\n",
      "2.0    104800\n",
      "1.0       960\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:01.955796Z",
     "start_time": "2025-11-08T13:45:01.870447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NOTE: it might be that the surveys are not really useful, a lot are close to 2\n",
    "df.describe()"
   ],
   "id": "1446d9d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        sample_index           time  pain_survey_1  pain_survey_2  \\\n",
       "count  105760.000000  105760.000000  105760.000000  105760.000000   \n",
       "mean      330.000000      79.500000       1.633746       1.654851   \n",
       "std       190.814948      46.187338       0.682423       0.669639   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%       165.000000      39.750000       2.000000       2.000000   \n",
       "50%       330.000000      79.500000       2.000000       2.000000   \n",
       "75%       495.000000     119.250000       2.000000       2.000000   \n",
       "max       660.000000     159.000000       2.000000       2.000000   \n",
       "\n",
       "       pain_survey_3  pain_survey_4         n_legs        n_hands  \\\n",
       "count  105760.000000  105760.000000  105760.000000  105760.000000   \n",
       "mean        1.653640       1.663134       1.990923       1.990923   \n",
       "std         0.666649       0.661994       0.094841       0.094841   \n",
       "min         0.000000       0.000000       1.000000       1.000000   \n",
       "25%         2.000000       2.000000       2.000000       2.000000   \n",
       "50%         2.000000       2.000000       2.000000       2.000000   \n",
       "75%         2.000000       2.000000       2.000000       2.000000   \n",
       "max         2.000000       2.000000       2.000000       2.000000   \n",
       "\n",
       "              n_eyes       joint_00  ...      joint_21      joint_22  \\\n",
       "count  105760.000000  105760.000000  ...  1.057600e+05  1.057600e+05   \n",
       "mean        1.990923       0.943095  ...  3.972126e-05  4.176794e-05   \n",
       "std         0.094841       0.202051  ...  4.974496e-03  5.472244e-03   \n",
       "min         1.000000       0.000000  ...  0.000000e+00  1.510494e-07   \n",
       "25%         2.000000       0.828277  ...  6.545878e-08  3.321650e-07   \n",
       "50%         2.000000       1.005126  ...  8.302747e-07  1.095971e-06   \n",
       "75%         2.000000       1.081039  ...  2.800090e-06  3.079464e-06   \n",
       "max         2.000000       1.407968  ...  1.442198e+00  1.305001e+00   \n",
       "\n",
       "           joint_23      joint_24      joint_25       joint_26       joint_27  \\\n",
       "count  1.057600e+05  1.057600e+05  1.057600e+05  105760.000000  105760.000000   \n",
       "mean   3.561780e-05  3.138109e-05  1.024604e-04       0.041905       0.058244   \n",
       "std    1.235449e-03  4.062914e-04  3.206128e-03       0.060293       0.079819   \n",
       "min    0.000000e+00  1.063144e-08  0.000000e+00       0.000203       0.000000   \n",
       "25%    3.275038e-07  2.841805e-07  7.161332e-07       0.009885       0.012652   \n",
       "50%    1.024209e-06  8.746148e-07  3.126723e-06       0.021898       0.031739   \n",
       "75%    3.021830e-06  2.507548e-06  9.946107e-06       0.048579       0.071051   \n",
       "max    2.742411e-01  3.643074e-02  9.473540e-01       1.223617       1.187419   \n",
       "\n",
       "            joint_28       joint_29  joint_30  \n",
       "count  105760.000000  105760.000000  105760.0  \n",
       "mean        0.049886       0.062273       0.5  \n",
       "std         0.060773       0.072597       0.0  \n",
       "min         0.000000       0.000000       0.5  \n",
       "25%         0.016290       0.019638       0.5  \n",
       "50%         0.031843       0.039041       0.5  \n",
       "75%         0.058741       0.079518       0.5  \n",
       "max         1.412037       1.370765       0.5  \n",
       "\n",
       "[8 rows x 40 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>n_legs</th>\n",
       "      <th>n_hands</th>\n",
       "      <th>n_eyes</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_21</th>\n",
       "      <th>joint_22</th>\n",
       "      <th>joint_23</th>\n",
       "      <th>joint_24</th>\n",
       "      <th>joint_25</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "      <th>joint_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.057600e+05</td>\n",
       "      <td>1.057600e+05</td>\n",
       "      <td>1.057600e+05</td>\n",
       "      <td>1.057600e+05</td>\n",
       "      <td>1.057600e+05</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.000000</td>\n",
       "      <td>105760.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>330.000000</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>1.633746</td>\n",
       "      <td>1.654851</td>\n",
       "      <td>1.653640</td>\n",
       "      <td>1.663134</td>\n",
       "      <td>1.990923</td>\n",
       "      <td>1.990923</td>\n",
       "      <td>1.990923</td>\n",
       "      <td>0.943095</td>\n",
       "      <td>...</td>\n",
       "      <td>3.972126e-05</td>\n",
       "      <td>4.176794e-05</td>\n",
       "      <td>3.561780e-05</td>\n",
       "      <td>3.138109e-05</td>\n",
       "      <td>1.024604e-04</td>\n",
       "      <td>0.041905</td>\n",
       "      <td>0.058244</td>\n",
       "      <td>0.049886</td>\n",
       "      <td>0.062273</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>190.814948</td>\n",
       "      <td>46.187338</td>\n",
       "      <td>0.682423</td>\n",
       "      <td>0.669639</td>\n",
       "      <td>0.666649</td>\n",
       "      <td>0.661994</td>\n",
       "      <td>0.094841</td>\n",
       "      <td>0.094841</td>\n",
       "      <td>0.094841</td>\n",
       "      <td>0.202051</td>\n",
       "      <td>...</td>\n",
       "      <td>4.974496e-03</td>\n",
       "      <td>5.472244e-03</td>\n",
       "      <td>1.235449e-03</td>\n",
       "      <td>4.062914e-04</td>\n",
       "      <td>3.206128e-03</td>\n",
       "      <td>0.060293</td>\n",
       "      <td>0.079819</td>\n",
       "      <td>0.060773</td>\n",
       "      <td>0.072597</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.510494e-07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.063144e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>165.000000</td>\n",
       "      <td>39.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.828277</td>\n",
       "      <td>...</td>\n",
       "      <td>6.545878e-08</td>\n",
       "      <td>3.321650e-07</td>\n",
       "      <td>3.275038e-07</td>\n",
       "      <td>2.841805e-07</td>\n",
       "      <td>7.161332e-07</td>\n",
       "      <td>0.009885</td>\n",
       "      <td>0.012652</td>\n",
       "      <td>0.016290</td>\n",
       "      <td>0.019638</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>330.000000</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.005126</td>\n",
       "      <td>...</td>\n",
       "      <td>8.302747e-07</td>\n",
       "      <td>1.095971e-06</td>\n",
       "      <td>1.024209e-06</td>\n",
       "      <td>8.746148e-07</td>\n",
       "      <td>3.126723e-06</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0.031739</td>\n",
       "      <td>0.031843</td>\n",
       "      <td>0.039041</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>495.000000</td>\n",
       "      <td>119.250000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.081039</td>\n",
       "      <td>...</td>\n",
       "      <td>2.800090e-06</td>\n",
       "      <td>3.079464e-06</td>\n",
       "      <td>3.021830e-06</td>\n",
       "      <td>2.507548e-06</td>\n",
       "      <td>9.946107e-06</td>\n",
       "      <td>0.048579</td>\n",
       "      <td>0.071051</td>\n",
       "      <td>0.058741</td>\n",
       "      <td>0.079518</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>660.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.407968</td>\n",
       "      <td>...</td>\n",
       "      <td>1.442198e+00</td>\n",
       "      <td>1.305001e+00</td>\n",
       "      <td>2.742411e-01</td>\n",
       "      <td>3.643074e-02</td>\n",
       "      <td>9.473540e-01</td>\n",
       "      <td>1.223617</td>\n",
       "      <td>1.187419</td>\n",
       "      <td>1.412037</td>\n",
       "      <td>1.370765</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 40 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "id": "67304d9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:02.019252Z",
     "start_time": "2025-11-08T13:45:02.012605Z"
    }
   },
   "source": [
    "# NOTE: joint_30 is constant -> useless\n",
    "df.drop(columns=[\"joint_30\"], inplace=True)"
   ],
   "outputs": [],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "id": "be3b5150",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:02.157075Z",
     "start_time": "2025-11-08T13:45:02.122278Z"
    }
   },
   "source": [
    "# Check in each sample index if the survey value changes\n",
    "df.groupby(\"sample_index\").var()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                     time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
       "sample_index                                                             \n",
       "0             2146.666667       0.578459       0.453420       0.370912   \n",
       "1             2146.666667       0.518239       0.427634       0.520597   \n",
       "2             2146.666667       0.232665       0.303420       0.348703   \n",
       "3             2146.666667       0.270401       0.451415       0.410024   \n",
       "4             2146.666667       0.367885       0.349017       0.417256   \n",
       "...                   ...            ...            ...            ...   \n",
       "656           2146.666667       0.384866       0.463994       0.482351   \n",
       "657           2146.666667       0.614937       0.387421       0.527319   \n",
       "658           2146.666667       0.400000       0.447170       0.308019   \n",
       "659           2146.666667       0.301258       0.392099       0.361281   \n",
       "660           2146.666667       0.534237       0.438836       0.526690   \n",
       "\n",
       "              pain_survey_4  n_legs  n_hands  n_eyes  joint_00  joint_01  ...  \\\n",
       "sample_index                                                              ...   \n",
       "0                  0.341785     0.0      0.0     0.0  0.002779  0.002312  ...   \n",
       "1                  0.285181     0.0      0.0     0.0  0.002029  0.002002  ...   \n",
       "2                  0.590212     0.0      0.0     0.0  0.003289  0.002869  ...   \n",
       "3                  0.389308     0.0      0.0     0.0  0.002848  0.002170  ...   \n",
       "4                  0.560377     0.0      0.0     0.0  0.001996  0.001848  ...   \n",
       "...                     ...     ...      ...     ...       ...       ...  ...   \n",
       "656                0.382036     0.0      0.0     0.0  0.002239  0.002606  ...   \n",
       "657                0.465252     0.0      0.0     0.0  0.013406  0.010557  ...   \n",
       "658                0.564111     0.0      0.0     0.0  0.002130  0.002614  ...   \n",
       "659                0.272602     0.0      0.0     0.0  0.001928  0.002839  ...   \n",
       "660                0.399017     0.0      0.0     0.0  0.002629  0.002081  ...   \n",
       "\n",
       "                  joint_20      joint_21      joint_22      joint_23  \\\n",
       "sample_index                                                           \n",
       "0             2.765450e-09  1.121023e-08  1.030416e-10  3.507136e-11   \n",
       "1             1.202636e-10  1.203627e-12  9.048195e-13  1.132519e-12   \n",
       "2             2.271433e-10  3.516154e-11  7.492228e-10  6.843855e-09   \n",
       "3             6.908745e-11  2.097331e-12  1.303198e-12  1.280051e-12   \n",
       "4             9.357456e-11  1.002201e-12  1.793468e-12  1.359855e-12   \n",
       "...                    ...           ...           ...           ...   \n",
       "656           3.781578e-06  2.390680e-09  1.236089e-08  8.372293e-08   \n",
       "657           4.199002e-05  1.540430e-07  3.065419e-08  4.670099e-07   \n",
       "658           1.071635e-10  5.172513e-12  7.039511e-12  7.257495e-12   \n",
       "659           6.925450e-11  3.340236e-12  6.567325e-12  6.956377e-12   \n",
       "660           7.784416e-11  1.026504e-12  1.105003e-12  1.074981e-12   \n",
       "\n",
       "                  joint_24      joint_25  joint_26  joint_27  joint_28  \\\n",
       "sample_index                                                             \n",
       "0             4.065838e-11  1.399554e-09  0.000316  0.000213  0.000239   \n",
       "1             7.930838e-13  8.677363e-12  0.000704  0.001049  0.005088   \n",
       "2             1.991356e-09  1.672197e-08  0.024022  0.009626  0.000732   \n",
       "3             7.887789e-13  9.435219e-11  0.000492  0.001027  0.000641   \n",
       "4             2.161616e-12  6.887033e-12  0.000216  0.000199  0.000043   \n",
       "...                    ...           ...       ...       ...       ...   \n",
       "656           2.021945e-08  4.956993e-07  0.000004  0.000005  0.000016   \n",
       "657           3.461188e-07  2.265925e-06  0.000382  0.000645  0.000263   \n",
       "658           2.155900e-12  1.572881e-10  0.000746  0.001668  0.001004   \n",
       "659           1.443975e-12  2.535323e-10  0.000373  0.000236  0.000633   \n",
       "660           7.787508e-13  6.221027e-12  0.000475  0.000385  0.018251   \n",
       "\n",
       "              joint_29  \n",
       "sample_index            \n",
       "0             0.000336  \n",
       "1             0.001512  \n",
       "2             0.000477  \n",
       "3             0.000489  \n",
       "4             0.000103  \n",
       "...                ...  \n",
       "656           0.000021  \n",
       "657           0.003186  \n",
       "658           0.000875  \n",
       "659           0.002056  \n",
       "660           0.001960  \n",
       "\n",
       "[661 rows x 38 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>n_legs</th>\n",
       "      <th>n_hands</th>\n",
       "      <th>n_eyes</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>joint_01</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_20</th>\n",
       "      <th>joint_21</th>\n",
       "      <th>joint_22</th>\n",
       "      <th>joint_23</th>\n",
       "      <th>joint_24</th>\n",
       "      <th>joint_25</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2146.666667</td>\n",
       "      <td>0.578459</td>\n",
       "      <td>0.453420</td>\n",
       "      <td>0.370912</td>\n",
       "      <td>0.341785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>...</td>\n",
       "      <td>2.765450e-09</td>\n",
       "      <td>1.121023e-08</td>\n",
       "      <td>1.030416e-10</td>\n",
       "      <td>3.507136e-11</td>\n",
       "      <td>4.065838e-11</td>\n",
       "      <td>1.399554e-09</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2146.666667</td>\n",
       "      <td>0.518239</td>\n",
       "      <td>0.427634</td>\n",
       "      <td>0.520597</td>\n",
       "      <td>0.285181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002029</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>...</td>\n",
       "      <td>1.202636e-10</td>\n",
       "      <td>1.203627e-12</td>\n",
       "      <td>9.048195e-13</td>\n",
       "      <td>1.132519e-12</td>\n",
       "      <td>7.930838e-13</td>\n",
       "      <td>8.677363e-12</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.005088</td>\n",
       "      <td>0.001512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2146.666667</td>\n",
       "      <td>0.232665</td>\n",
       "      <td>0.303420</td>\n",
       "      <td>0.348703</td>\n",
       "      <td>0.590212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>...</td>\n",
       "      <td>2.271433e-10</td>\n",
       "      <td>3.516154e-11</td>\n",
       "      <td>7.492228e-10</td>\n",
       "      <td>6.843855e-09</td>\n",
       "      <td>1.991356e-09</td>\n",
       "      <td>1.672197e-08</td>\n",
       "      <td>0.024022</td>\n",
       "      <td>0.009626</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>0.000477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2146.666667</td>\n",
       "      <td>0.270401</td>\n",
       "      <td>0.451415</td>\n",
       "      <td>0.410024</td>\n",
       "      <td>0.389308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002848</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>...</td>\n",
       "      <td>6.908745e-11</td>\n",
       "      <td>2.097331e-12</td>\n",
       "      <td>1.303198e-12</td>\n",
       "      <td>1.280051e-12</td>\n",
       "      <td>7.887789e-13</td>\n",
       "      <td>9.435219e-11</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.000489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2146.666667</td>\n",
       "      <td>0.367885</td>\n",
       "      <td>0.349017</td>\n",
       "      <td>0.417256</td>\n",
       "      <td>0.560377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>...</td>\n",
       "      <td>9.357456e-11</td>\n",
       "      <td>1.002201e-12</td>\n",
       "      <td>1.793468e-12</td>\n",
       "      <td>1.359855e-12</td>\n",
       "      <td>2.161616e-12</td>\n",
       "      <td>6.887033e-12</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>2146.666667</td>\n",
       "      <td>0.384866</td>\n",
       "      <td>0.463994</td>\n",
       "      <td>0.482351</td>\n",
       "      <td>0.382036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>...</td>\n",
       "      <td>3.781578e-06</td>\n",
       "      <td>2.390680e-09</td>\n",
       "      <td>1.236089e-08</td>\n",
       "      <td>8.372293e-08</td>\n",
       "      <td>2.021945e-08</td>\n",
       "      <td>4.956993e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>2146.666667</td>\n",
       "      <td>0.614937</td>\n",
       "      <td>0.387421</td>\n",
       "      <td>0.527319</td>\n",
       "      <td>0.465252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013406</td>\n",
       "      <td>0.010557</td>\n",
       "      <td>...</td>\n",
       "      <td>4.199002e-05</td>\n",
       "      <td>1.540430e-07</td>\n",
       "      <td>3.065419e-08</td>\n",
       "      <td>4.670099e-07</td>\n",
       "      <td>3.461188e-07</td>\n",
       "      <td>2.265925e-06</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.003186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>2146.666667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.447170</td>\n",
       "      <td>0.308019</td>\n",
       "      <td>0.564111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>...</td>\n",
       "      <td>1.071635e-10</td>\n",
       "      <td>5.172513e-12</td>\n",
       "      <td>7.039511e-12</td>\n",
       "      <td>7.257495e-12</td>\n",
       "      <td>2.155900e-12</td>\n",
       "      <td>1.572881e-10</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.000875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>2146.666667</td>\n",
       "      <td>0.301258</td>\n",
       "      <td>0.392099</td>\n",
       "      <td>0.361281</td>\n",
       "      <td>0.272602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>0.002839</td>\n",
       "      <td>...</td>\n",
       "      <td>6.925450e-11</td>\n",
       "      <td>3.340236e-12</td>\n",
       "      <td>6.567325e-12</td>\n",
       "      <td>6.956377e-12</td>\n",
       "      <td>1.443975e-12</td>\n",
       "      <td>2.535323e-10</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.002056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>2146.666667</td>\n",
       "      <td>0.534237</td>\n",
       "      <td>0.438836</td>\n",
       "      <td>0.526690</td>\n",
       "      <td>0.399017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.002081</td>\n",
       "      <td>...</td>\n",
       "      <td>7.784416e-11</td>\n",
       "      <td>1.026504e-12</td>\n",
       "      <td>1.105003e-12</td>\n",
       "      <td>1.074981e-12</td>\n",
       "      <td>7.787508e-13</td>\n",
       "      <td>6.221027e-12</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.018251</td>\n",
       "      <td>0.001960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>661 rows × 38 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:02.294784Z",
     "start_time": "2025-11-08T13:45:02.225736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Visualise the count of timestamps for each activity\n",
    "plt.figure(figsize=(17, 5))\n",
    "sns.countplot(\n",
    "    x='label',\n",
    "    data=df_labels,\n",
    "    order=df_labels['label'].value_counts().index,\n",
    "    palette='tab10'\n",
    ")\n",
    "\n",
    "# Set the title of the plot\n",
    "plt.title('Pain Scale Timestamps')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ],
   "id": "df1a2435ff3f1977",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1700x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:02.358280Z",
     "start_time": "2025-11-08T13:45:02.355476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# All sample_indexes have the same number of timesteps\n",
    "df.groupby('sample_index').size().nunique() == 1"
   ],
   "id": "87e29a2a47d502b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:02.465329Z",
     "start_time": "2025-11-08T13:45:02.457796Z"
    }
   },
   "cell_type": "code",
   "source": "df = df.merge(df_labels, on='sample_index', how='left')",
   "id": "f5cc9d24555e3dc",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:02.581549Z",
     "start_time": "2025-11-08T13:45:02.571548Z"
    }
   },
   "cell_type": "code",
   "source": "df.head(5)",
   "id": "a8e3e3548bf1a980",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
       "0             0     0            2.0            0.0            2.0   \n",
       "1             0     1            2.0            2.0            2.0   \n",
       "2             0     2            2.0            0.0            2.0   \n",
       "3             0     3            2.0            2.0            2.0   \n",
       "4             0     4            2.0            2.0            2.0   \n",
       "\n",
       "   pain_survey_4  n_legs  n_hands  n_eyes  joint_00  ...      joint_21  \\\n",
       "0            1.0     2.0      2.0     2.0  1.094705  ...  3.499558e-06   \n",
       "1            2.0     2.0      2.0     2.0  1.135183  ...  3.976952e-07   \n",
       "2            2.0     2.0      2.0     2.0  1.080745  ...  1.533820e-07   \n",
       "3            2.0     2.0      2.0     2.0  0.938017  ...  1.006865e-05   \n",
       "4            2.0     2.0      2.0     2.0  1.090185  ...  4.437265e-06   \n",
       "\n",
       "       joint_22  joint_23      joint_24  joint_25  joint_26  joint_27  \\\n",
       "0  1.945042e-06  0.000004  1.153299e-05  0.000004  0.017592  0.013508   \n",
       "1  6.765108e-07  0.000006  4.643774e-08  0.000000  0.013352  0.000000   \n",
       "2  1.698525e-07  0.000001  2.424536e-06  0.000003  0.016225  0.008110   \n",
       "3  5.511079e-07  0.000002  5.432416e-08  0.000000  0.011832  0.007450   \n",
       "4  1.735459e-07  0.000002  5.825366e-08  0.000007  0.005360  0.002532   \n",
       "\n",
       "   joint_28  joint_29    label  \n",
       "0  0.026798  0.027815  no_pain  \n",
       "1  0.013377  0.013716  no_pain  \n",
       "2  0.024097  0.023105  no_pain  \n",
       "3  0.028613  0.024648  no_pain  \n",
       "4  0.033026  0.025328  no_pain  \n",
       "\n",
       "[5 rows x 40 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>n_legs</th>\n",
       "      <th>n_hands</th>\n",
       "      <th>n_eyes</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_21</th>\n",
       "      <th>joint_22</th>\n",
       "      <th>joint_23</th>\n",
       "      <th>joint_24</th>\n",
       "      <th>joint_25</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.094705</td>\n",
       "      <td>...</td>\n",
       "      <td>3.499558e-06</td>\n",
       "      <td>1.945042e-06</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1.153299e-05</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.017592</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>0.026798</td>\n",
       "      <td>0.027815</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.135183</td>\n",
       "      <td>...</td>\n",
       "      <td>3.976952e-07</td>\n",
       "      <td>6.765108e-07</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>4.643774e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013377</td>\n",
       "      <td>0.013716</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.080745</td>\n",
       "      <td>...</td>\n",
       "      <td>1.533820e-07</td>\n",
       "      <td>1.698525e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>2.424536e-06</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.016225</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.024097</td>\n",
       "      <td>0.023105</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.938017</td>\n",
       "      <td>...</td>\n",
       "      <td>1.006865e-05</td>\n",
       "      <td>5.511079e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>5.432416e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011832</td>\n",
       "      <td>0.007450</td>\n",
       "      <td>0.028613</td>\n",
       "      <td>0.024648</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.090185</td>\n",
       "      <td>...</td>\n",
       "      <td>4.437265e-06</td>\n",
       "      <td>1.735459e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>5.825366e-08</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.005360</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.033026</td>\n",
       "      <td>0.025328</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:04.334518Z",
     "start_time": "2025-11-08T13:45:02.685386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print the number of unique activity executions\n",
    "print(f'The dataset is composed of {df[\"sample_index\"].nunique()} different executions')\n",
    "\n",
    "# Count the unique IDs for distinct activity executions\n",
    "n_users = len(df['sample_index'].unique())\n",
    "\n",
    "# Create a custom colour map for better distinction of unique IDs\n",
    "colors = plt.cm.turbo(np.linspace(0, 1, n_users))\n",
    "\n",
    "# Visualise the count of timestamps per unique ID\n",
    "plt.figure(figsize=(17, 5))\n",
    "sns.countplot(\n",
    "    x='sample_index',\n",
    "    data=df,\n",
    "    order=df['sample_index'].value_counts().index,\n",
    "    palette=colors\n",
    ")\n",
    "\n",
    "# Set the title of the plot and disable x-axis labels for clarity\n",
    "plt.title('Per Id Timestamps')\n",
    "plt.xticks([], [])  # Remove x-axis ticks and labels\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ],
   "id": "7bb0ca2f2eafd8b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is composed of 661 different executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1700x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:04.442532Z",
     "start_time": "2025-11-08T13:45:04.441039Z"
    }
   },
   "cell_type": "code",
   "source": "scale_columns = df.columns [2:-1]",
   "id": "41668e059f464b48",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:04.488377Z",
     "start_time": "2025-11-08T13:45:04.485450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a function to build sequences from the dataset\n",
    "def build_sequences(df, window=200, stride=200):\n",
    "    # Sanity check to ensure the window is divisible by the stride\n",
    "    assert window % stride == 0\n",
    "\n",
    "    # Initialise lists to store sequences and their corresponding labels\n",
    "    dataset = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate over unique IDs in the DataFrame\n",
    "    for id in df['sample_index'].unique():\n",
    "        # Extract sensor data for the current ID\n",
    "        temp = df[df['sample_index'] == id][scale_columns].values\n",
    "\n",
    "        # Retrieve the activity label for the current ID\n",
    "        label = df[df['sample_index'] == id]['label'].values[0]\n",
    "\n",
    "        # Calculate padding length to ensure full windows\n",
    "        padding_len = window - len(temp) % window\n",
    "\n",
    "        # Create zero padding and concatenate with the data\n",
    "        padding = np.zeros((padding_len, len(scale_columns)), dtype='float32')\n",
    "        temp = np.concatenate((temp, padding))\n",
    "\n",
    "        # Build feature windows and associate them with labels\n",
    "        idx = 0\n",
    "        while idx + window <= len(temp):\n",
    "            dataset.append(temp[idx:idx + window])\n",
    "            labels.append(label)\n",
    "            idx += stride\n",
    "\n",
    "    # Convert lists to numpy arrays for further processing\n",
    "    dataset = np.array(dataset)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return dataset, labels"
   ],
   "id": "777747b57f0b5d13",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:04.533612Z",
     "start_time": "2025-11-08T13:45:04.531224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    # Determine optimal number of worker processes for data loading\n",
    "    cpu_cores = os.cpu_count() or 2\n",
    "    num_workers = max(2, min(4, cpu_cores))\n",
    "\n",
    "    # Create DataLoader with performance optimizations\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,  # Faster GPU transfer\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=4,  # Load 4 batches ahead\n",
    "    )"
   ],
   "id": "86f6da99eaf630ce",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MODEL BUILDING",
   "id": "18c42851cd9d8efc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:04.581153Z",
     "start_time": "2025-11-08T13:45:04.576085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def recurrent_summary(model, input_size):\n",
    "    \"\"\"\n",
    "    Custom summary function that emulates torchinfo's output while correctly\n",
    "    counting parameters for RNN/GRU/LSTM layers.\n",
    "\n",
    "    This function is designed for models whose direct children are\n",
    "    nn.Linear, nn.RNN, nn.GRU, or nn.LSTM layers.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to analyze.\n",
    "        input_size (tuple): Shape of the input tensor (e.g., (seq_len, features)).\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary to store output shapes captured by forward hooks\n",
    "    output_shapes = {}\n",
    "    # List to track hook handles for later removal\n",
    "    hooks = []\n",
    "\n",
    "    def get_hook(name):\n",
    "        \"\"\"Factory function to create a forward hook for a specific module.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            # Handle RNN layer outputs (returns a tuple)\n",
    "            if isinstance(output, tuple):\n",
    "                # output[0]: all hidden states with shape (batch, seq_len, hidden*directions)\n",
    "                shape1 = list(output[0].shape)\n",
    "                shape1[0] = -1  # Replace batch dimension with -1\n",
    "\n",
    "                # output[1]: final hidden state h_n (or tuple (h_n, c_n) for LSTM)\n",
    "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
    "                    shape2 = list(output[1][0].shape)  # Extract h_n only\n",
    "                else:  # RNN/GRU case: h_n only\n",
    "                    shape2 = list(output[1].shape)\n",
    "\n",
    "                # Replace batch dimension (middle position) with -1\n",
    "                shape2[1] = -1\n",
    "\n",
    "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
    "\n",
    "            # Handle standard layer outputs (e.g., Linear)\n",
    "            else:\n",
    "                shape = list(output.shape)\n",
    "                shape[0] = -1  # Replace batch dimension with -1\n",
    "                output_shapes[name] = f\"{shape}\"\n",
    "        return hook\n",
    "\n",
    "    # 1. Determine the device where model parameters reside\n",
    "    try:\n",
    "        device = next(model.parameters()).device\n",
    "    except StopIteration:\n",
    "        device = torch.device(\"cpu\")  # Fallback for models without parameters\n",
    "\n",
    "    # 2. Create a dummy input tensor with batch_size=1\n",
    "    dummy_input = torch.randn(1, *input_size).to(device)\n",
    "\n",
    "    # 3. Register forward hooks on target layers\n",
    "    # Iterate through direct children of the model (e.g., self.rnn, self.classifier)\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
    "            # Register the hook and store its handle for cleanup\n",
    "            hook_handle = module.register_forward_hook(get_hook(name))\n",
    "            hooks.append(hook_handle)\n",
    "\n",
    "    # 4. Execute a dummy forward pass in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model(dummy_input)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dummy forward pass: {e}\")\n",
    "            # Clean up hooks even if an error occurs\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "            return\n",
    "\n",
    "    # 5. Remove all registered hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    # --- 6. Print the summary table ---\n",
    "\n",
    "    print(\"-\" * 79)\n",
    "    # Column headers\n",
    "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
    "    print(\"=\" * 79)\n",
    "\n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "\n",
    "    # Iterate through modules again to collect and display parameter information\n",
    "    for name, module in model.named_children():\n",
    "        if name in output_shapes:\n",
    "            # Count total and trainable parameters for this module\n",
    "            module_params = sum(p.numel() for p in module.parameters())\n",
    "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "            total_params += module_params\n",
    "            total_trainable_params += trainable_params\n",
    "\n",
    "            # Format strings for display\n",
    "            layer_name = f\"{name} ({type(module).__name__})\"\n",
    "            output_shape_str = str(output_shapes[name])\n",
    "            params_str = f\"{trainable_params:,}\"\n",
    "\n",
    "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
    "\n",
    "    print(\"=\" * 79)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
    "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
    "    print(\"-\" * 79)"
   ],
   "id": "4c84d554144a5d35",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:04.626823Z",
     "start_time": "2025-11-08T13:45:04.623250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RecurrentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic RNN classifier (RNN, LSTM, GRU).\n",
    "    Uses the last hidden state for classification.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            num_classes,\n",
    "            rnn_type='GRU',        # 'RNN', 'LSTM', or 'GRU'\n",
    "            bidirectional=False,\n",
    "            dropout_rate=0.2\n",
    "            ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        # Map string name to PyTorch RNN class\n",
    "        rnn_map = {\n",
    "            'RNN': nn.RNN,\n",
    "            'LSTM': nn.LSTM,\n",
    "            'GRU': nn.GRU\n",
    "        }\n",
    "\n",
    "        if rnn_type not in rnn_map:\n",
    "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
    "\n",
    "        rnn_module = rnn_map[rnn_type]\n",
    "\n",
    "        # Dropout is only applied between layers (if num_layers > 1)\n",
    "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
    "\n",
    "        # Create the recurrent layer\n",
    "        self.rnn = rnn_module(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,       # Input shape: (batch, seq_len, features)\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_val\n",
    "        )\n",
    "\n",
    "        # Calculate input size for the final classifier\n",
    "        if self.bidirectional:\n",
    "            classifier_input_size = hidden_size * 2 # Concat fwd + bwd\n",
    "        else:\n",
    "            classifier_input_size = hidden_size\n",
    "\n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Linear(classifier_input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, seq_length, input_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # rnn_out shape: (batch_size, seq_len, hidden_size * num_directions)\n",
    "        rnn_out, hidden = self.rnn(x)\n",
    "\n",
    "        # LSTM returns (h_n, c_n), we only need h_n\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            hidden = hidden[0]\n",
    "\n",
    "        # hidden shape: (num_layers * num_directions, batch_size, hidden_size)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            # Reshape to (num_layers, 2, batch_size, hidden_size)\n",
    "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
    "\n",
    "            # Concat last fwd (hidden[-1, 0, ...]) and bwd (hidden[-1, 1, ...])\n",
    "            # Final shape: (batch_size, hidden_size * 2)\n",
    "            hidden_to_classify = torch.cat([hidden[-1, 0, :, :], hidden[-1, 1, :, :]], dim=1)\n",
    "        else:\n",
    "            # Take the last layer's hidden state\n",
    "            # Final shape: (batch_size, hidden_size)\n",
    "            hidden_to_classify = hidden[-1]\n",
    "\n",
    "        # Get logits\n",
    "        logits = self.classifier(hidden_to_classify)\n",
    "        return logits"
   ],
   "id": "6343d76cc20bf6c8",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:04.672745Z",
     "start_time": "2025-11-08T13:45:04.669245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
    "    \"\"\"\n",
    "    Perform one complete training epoch through the entire training dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train\n",
    "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
    "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
    "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
    "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "        l1_lambda (float): Lambda for L1 regularization\n",
    "        l2_lambda (float): Lambda for L2 regularization\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Iterate through training batches\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # Move data to device (GPU/CPU)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Clear gradients from previous step\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward pass with mixed precision (if CUDA available)\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            # Add L1 and L2 regularization\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "\n",
    "\n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Accumulate metrics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_f1"
   ],
   "id": "ede88e3edfdd3d50",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:04.718002Z",
     "start_time": "2025-11-08T13:45:04.715394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Perform one complete validation epoch through the entire validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n",
    "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
    "        criterion (nn.Module): Loss function used to calculate validation loss\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n",
    "\n",
    "    Note:\n",
    "        This function automatically sets the model to evaluation mode and disables\n",
    "        gradient computation for efficiency during validation.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Disable gradient computation for validation\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            # Move data to device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass with mixed precision (if CUDA available)\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, targets)\n",
    "\n",
    "            # Accumulate metrics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_accuracy = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_accuracy"
   ],
   "id": "2b9b18401705c003",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:04.762848Z",
     "start_time": "2025-11-08T13:45:04.760185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
    "    \"\"\"\n",
    "    Log training metrics and model parameters to TensorBoard for visualization.\n",
    "\n",
    "    Args:\n",
    "        writer (SummaryWriter): TensorBoard SummaryWriter object for logging\n",
    "        epoch (int): Current epoch number (used as x-axis in TensorBoard plots)\n",
    "        train_loss (float): Training loss for this epoch\n",
    "        train_f1 (float): Training f1 score for this epoch\n",
    "        val_loss (float): Validation loss for this epoch\n",
    "        val_f1 (float): Validation f1 score for this epoch\n",
    "        model (nn.Module): The neural network model (for logging weights/gradients)\n",
    "\n",
    "    Note:\n",
    "        This function logs scalar metrics (loss/f1 score) and histograms of model\n",
    "        parameters and gradients, which helps monitor training progress and detect\n",
    "        issues like vanishing/exploding gradients.\n",
    "    \"\"\"\n",
    "    # Log scalar metrics\n",
    "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
    "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
    "\n",
    "    # Log model parameters and gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            # Check if the tensor is not empty before adding a histogram\n",
    "            if param.numel() > 0:\n",
    "                writer.add_histogram(f'{name}/weights', param.data, epoch)\n",
    "            if param.grad is not None:\n",
    "                # Check if the gradient tensor is not empty before adding a histogram\n",
    "                if param.grad.numel() > 0:\n",
    "                    if param.grad is not None and torch.isfinite(param.grad).all():\n",
    "                        writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)"
   ],
   "id": "d685e5c47b750748",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:04.809708Z",
     "start_time": "2025-11-08T13:45:04.805213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
    "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
    "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
    "    \"\"\"\n",
    "    Train the neural network model on the training data and validate on the validation data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train\n",
    "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
    "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
    "        epochs (int): Number of training epochs\n",
    "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
    "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
    "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "        l1_lambda (float): L1 regularization coefficient (default: 0)\n",
    "        l2_lambda (float): L2 regularization coefficient (default: 0)\n",
    "        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n",
    "        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n",
    "        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n",
    "        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n",
    "        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n",
    "        verbose (int, optional): Frequency of printing training progress (default: 10)\n",
    "        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n",
    "\n",
    "    Returns:\n",
    "        tuple: (model, training_history) - Trained model and metrics history\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize metrics tracking\n",
    "    training_history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "\n",
    "    # Configure early stopping if patience is set\n",
    "    if patience > 0:\n",
    "        patience_counter = 0\n",
    "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "    print(f\"Training {epochs} epochs...\")\n",
    "\n",
    "    # Main training loop: iterate through epochs\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        # Forward pass through training data, compute gradients, update weights\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
    "        )\n",
    "\n",
    "        # Evaluate model on validation data without updating weights\n",
    "        val_loss, val_f1 = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        # Store metrics for plotting and analysis\n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        training_history['val_loss'].append(val_loss)\n",
    "        training_history['train_f1'].append(train_f1)\n",
    "        training_history['val_f1'].append(val_f1)\n",
    "\n",
    "        # Write metrics to TensorBoard for visualization\n",
    "        if writer is not None:\n",
    "            log_metrics_to_tensorboard(\n",
    "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
    "            )\n",
    "\n",
    "        # Print progress every N epochs or on first epoch\n",
    "        if verbose > 0:\n",
    "            if epoch % verbose == 0 or epoch == 1:\n",
    "                print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                    f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n",
    "                    f\"Val: Loss={val_loss:.4f}, F1 Score={val_f1:.4f}\")\n",
    "\n",
    "        # Early stopping logic: monitor metric and save best model\n",
    "        if patience > 0:\n",
    "            current_metric = training_history[evaluation_metric][-1]\n",
    "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "    # Restore best model weights if early stopping was used\n",
    "    if restore_best_weights and patience > 0:\n",
    "        model.load_state_dict(torch.load(\"models/\"+experiment_name+'_model.pt'))\n",
    "        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
    "\n",
    "    # Save final model if no early stopping\n",
    "    if patience == 0:\n",
    "        torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
    "\n",
    "    # Close TensorBoard writer\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "    return model, training_history"
   ],
   "id": "31b8d0a71494b339",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:04.855184Z",
     "start_time": "2025-11-08T13:45:04.852460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_val_loss_and_f1(training_history):\n",
    "    # @title Plot Hitory\n",
    "    # Create a figure with two side-by-side subplots (two columns)\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
    "\n",
    "    # Plot of training and validation loss on the first axis\n",
    "    ax1.plot(training_history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
    "    ax1.plot(training_history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n",
    "    ax1.set_title('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "\n",
    "    # Plot of training and validation accuracy on the second axis\n",
    "    ax2.plot(training_history['train_f1'], label='Training f1', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
    "    ax2.plot(training_history['val_f1'], label='Validation f1', alpha=0.9, color='#ff7f0e')\n",
    "    ax2.set_title('F1 Score')\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3)\n",
    "\n",
    "    # Adjust the layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(right=0.85)\n",
    "    plt.show()"
   ],
   "id": "e8e1c3f76c78ffe6",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:04.901538Z",
     "start_time": "2025-11-08T13:45:04.898324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def val_confusion_matrix(val_loader, model):\n",
    "    # @title Plot Confusion Matrix\n",
    "    # Collect predictions and ground truth labels\n",
    "    val_preds, val_targets = [], []\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "\n",
    "            # Forward pass: get model predictions\n",
    "            logits = model(xb)\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            # Store batch results\n",
    "            val_preds.append(preds)\n",
    "            val_targets.append(yb.numpy())\n",
    "\n",
    "    # Combine all batches into single arrays\n",
    "    val_preds = np.concatenate(val_preds)\n",
    "    val_targets = np.concatenate(val_targets)\n",
    "\n",
    "    # Calculate overall validation metrics\n",
    "    val_acc = accuracy_score(val_targets, val_preds)\n",
    "    val_prec = precision_score(val_targets, val_preds, average='weighted')\n",
    "    val_rec = recall_score(val_targets, val_preds, average='weighted')\n",
    "    val_f1 = f1_score(val_targets, val_preds, average='weighted')\n",
    "    print(f\"Accuracy over the validation set: {val_acc:.4f}\")\n",
    "    print(f\"Precision over the validation set: {val_prec:.4f}\")\n",
    "    print(f\"Recall over the validation set: {val_rec:.4f}\")\n",
    "    print(f\"F1 score over the validation set: {val_f1:.4f}\")\n",
    "\n",
    "    # Generate confusion matrix for detailed error analysis\n",
    "    cm = confusion_matrix(val_targets, val_preds)\n",
    "\n",
    "    # Create numeric labels for heatmap annotation\n",
    "    labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n",
    "\n",
    "    # Visualise confusion matrix\n",
    "    plt.figure(figsize=(8, 7))\n",
    "    sns.heatmap(cm, annot=labels, fmt='',\n",
    "                cmap='Blues')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix — Validation Set')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "39303746b9ab2977",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:04.950727Z",
     "start_time": "2025-11-08T13:45:04.947949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a function to build sequences from the dataset\n",
    "def build_sequences_test(df, window=200, stride=200):\n",
    "    # Sanity check to ensure the window is divisible by the stride\n",
    "    assert window % stride == 0\n",
    "\n",
    "    # Initialise lists to store sequences and their corresponding labels\n",
    "    dataset = []\n",
    "    sample_indices = []\n",
    "\n",
    "    # Iterate over unique IDs in the DataFrame\n",
    "    for id in df['sample_index'].unique():\n",
    "        # Extract sensor data for the current ID\n",
    "        temp = df[df['sample_index'] == id][scale_columns].values\n",
    "\n",
    "        # Calculate padding length to ensure full windows\n",
    "        padding_len = window - len(temp) % window\n",
    "\n",
    "        # Create zero padding and concatenate with the data\n",
    "        padding = np.zeros((padding_len, len(scale_columns)), dtype='float32')\n",
    "        temp = np.concatenate((temp, padding))\n",
    "\n",
    "        # Build feature windows and associate them with labels\n",
    "        idx = 0\n",
    "        while idx + window <= len(temp):\n",
    "            dataset.append(temp[idx:idx + window])\n",
    "            idx += stride\n",
    "            sample_indices.append(id)\n",
    "\n",
    "    # Convert lists to numpy arrays for further processing\n",
    "    dataset = np.array(dataset)\n",
    "\n",
    "    return dataset, sample_indices"
   ],
   "id": "a11b2993143a0890",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:04.995562Z",
     "start_time": "2025-11-08T13:45:04.994021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Update best model if current performance is superior\n",
    "#if training_history['val_f1'][-1] > best_performance:\n",
    "#    best_model = rnn_model\n",
    "#    best_performance = training_history['val_f1'][-1]"
   ],
   "id": "c52119bd972119cd",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:05.619610Z",
     "start_time": "2025-11-08T13:45:05.038046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_test = pd.read_csv('pirate_pain_test.csv', header=0)\n",
    "\n",
    "float_cols = df_test.select_dtypes(include=['float64', 'int64']).columns[2:]\n",
    "for col in float_cols:\n",
    "    df_test[col] = df_test[col].astype(np.float32)\n",
    "\n",
    "df_test.drop(columns=[\"n_legs\", \"n_hands\", \"n_eyes\"], inplace=True)\n",
    "df_test.drop(columns=[\"joint_30\"], inplace=True)"
   ],
   "id": "84ce7b07bf8f332b",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:05.626130Z",
     "start_time": "2025-11-08T13:45:05.623209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def avg_logits_test(model, test_loader, sample_indices):\n",
    "    logits_val = []\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        for xb, in test_loader:\n",
    "            xb = xb.to(device)\n",
    "\n",
    "            # Forward pass: get model predictions\n",
    "            logits = model(xb)\n",
    "            logits_val.append(logits.cpu().numpy())\n",
    "\n",
    "    # Combine all batches\n",
    "    logits_val = np.concatenate(logits_val, axis=0)  # shape: [num_windows, num_classes]\n",
    "\n",
    "    # sample_indices should be same length as logits_val\n",
    "    df_results = pd.DataFrame({\n",
    "        \"sample_index\": sample_indices,\n",
    "        \"logits\": list(logits_val)\n",
    "    })\n",
    "\n",
    "    # Average logits per sample\n",
    "    df_mean_logits = df_results.groupby(\"sample_index\")[\"logits\"].apply(\n",
    "        lambda arrs: np.mean(np.vstack(arrs), axis=0)\n",
    "    )\n",
    "\n",
    "    # Final predictions = argmax of mean logits\n",
    "    final_preds = np.array([logit_vec.argmax() for logit_vec in df_mean_logits])\n",
    "\n",
    "    # Apply mapping to your final_preds\n",
    "    test_labels = [inv_label_mapping[p] for p in final_preds]\n",
    "\n",
    "    df_mean = pd.DataFrame({\n",
    "    \"sample_index\": df_mean_logits.index,\n",
    "    \"labels\": test_labels\n",
    "    })\n",
    "\n",
    "    return df_mean"
   ],
   "id": "90c02f60515df3cb",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:05.671859Z",
     "start_time": "2025-11-08T13:45:05.669201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def last_logits_test(model, test_loader, sample_indices):\n",
    "    logits_val = []\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        for xb, in test_loader:\n",
    "            xb = xb.to(device)\n",
    "\n",
    "            # Forward pass: get model predictions\n",
    "            logits = model(xb)\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            logits_val.append(logits.cpu().numpy())\n",
    "\n",
    "    # Combine all batches\n",
    "    logits_val = np.concatenate(logits_val, axis=0)  # shape: [num_windows, num_classes]\n",
    "\n",
    "    # sample_indices should be same length as logits_val\n",
    "    df_results = pd.DataFrame({\n",
    "        \"sample_index\": sample_indices,\n",
    "        \"logits\": list(logits_val)\n",
    "    })\n",
    "\n",
    "    df_last = df_results.groupby(\"sample_index\")[\"logits\"].apply(lambda arrs: arrs.iloc[-1])\n",
    "\n",
    "    # Final predictions = argmax of mean logits\n",
    "    final_preds = np.array([logit_vec.argmax() for logit_vec in df_last])\n",
    "\n",
    "    # Apply mapping to your final_preds\n",
    "    test_labels = [inv_label_mapping[p] for p in final_preds]\n",
    "\n",
    "    df_last = pd.DataFrame({\n",
    "        \"sample_index\": df_last.index,\n",
    "        \"labels\": test_labels\n",
    "    })\n",
    "\n",
    "    return df_last"
   ],
   "id": "659aaa602b95f419",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:05.718724Z",
     "start_time": "2025-11-08T13:45:05.717155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a mapping of activity names to integer labels\n",
    "label_mapping = {\n",
    "    'no_pain': 0,\n",
    "    'low_pain': 1,\n",
    "    'high_pain': 2,\n",
    "}\n",
    "\n",
    "# invert it so we can map int → string\n",
    "inv_label_mapping = {v: k for k, v in label_mapping.items()}\n"
   ],
   "id": "396d4edc8610808",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:05.765620Z",
     "start_time": "2025-11-08T13:45:05.762825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cross-validation\n",
    "K = 5                    # Number of splits (5 and 10 are considered good values)\n",
    "N_VAL_USERS = 100          # Number of users for validation split\n",
    "\n",
    "# Training\n",
    "EPOCHS = 500             # Maximum epochs (increase to improve performance)\n",
    "PATIENCE = 50            # Early stopping patience (increase to improve performance)\n",
    "VERBOSE = 10             # Print frequency\n",
    "\n",
    "# Optimisation\n",
    "LEARNING_RATE = 1e-3     # Learning rate\n",
    "BATCH_SIZE = 512         # Batch size\n",
    "WINDOW_SIZE = 20        # Input window size\n",
    "STRIDE = 5              # Input stride\n",
    "\n",
    "# Architecture\n",
    "HIDDEN_LAYERS = 2        # Hidden layers\n",
    "HIDDEN_SIZE = 128        # Neurons per layer\n",
    "RNN_TYPE = 'GRU'         # Type of RNN architecture\n",
    "BIDIRECTIONAL = False    # Bidirectional RNN\n",
    "\n",
    "EXPERIMENT_NAME = \"GRU MODEL\"\n",
    "\n",
    "# Regularisation\n",
    "DROPOUT_RATE = 0.2       # Dropout probability\n",
    "L1_LAMBDA = 0            # L1 penalty\n",
    "L2_LAMBDA = 0            # L2 penalty\n",
    "\n",
    "# Training utilities\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "id": "511f88d246620c2b",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:05.818519Z",
     "start_time": "2025-11-08T13:45:05.810935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def k_shuffle_split_cross_validation_round_rnn(df, epochs, criterion, device,\n",
    "                            k, n_val_users, batch_size, hidden_layers, hidden_size, learning_rate, dropout_rate,\n",
    "                            window_size, stride, rnn_type, bidirectional,\n",
    "                            l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
    "                            restore_best_weights=True, writer=None, verbose=10, seed=42, experiment_name=\"\"):\n",
    "    \"\"\"\n",
    "    Perform K-fold shuffle split cross-validation with user-based splitting for time series data.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with columns ['user_id', 'activity', 'x_axis', 'y_axis', 'z_axis', 'id']\n",
    "        epochs: Number of training epochs\n",
    "        criterion: Loss function\n",
    "        device: torch.device for computation\n",
    "        k: Number of cross-validation splits\n",
    "        n_val_users: Number of users for validation set\n",
    "        n_test_users: Number of users for test set\n",
    "        batch_size: Batch size for training\n",
    "        hidden_layers: Number of recurrent layers\n",
    "        hidden_size: Hidden state dimensionality\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        dropout_rate: Dropout rate\n",
    "        window_size: Length of sliding windows\n",
    "        stride: Step size for sliding windows\n",
    "        rnn_type: Type of RNN ('RNN', 'LSTM', 'GRU')\n",
    "        bidirectional: Whether to use bidirectional RNN\n",
    "        l1_lambda: L1 regularization coefficient (if used)\n",
    "        l2_lambda: L2 regularization coefficient (weight_decay)\n",
    "        patience: Early stopping patience\n",
    "        evaluation_metric: Metric to monitor for early stopping\n",
    "        mode: 'max' or 'min' for evaluation metric\n",
    "        restore_best_weights: Whether to restore best weights after training\n",
    "        writer: TensorBoard writer\n",
    "        verbose: Verbosity level\n",
    "        seed: Random seed\n",
    "        experiment_name: Name for experiment logging\n",
    "\n",
    "    Returns:\n",
    "        fold_losses: Dict with validation losses for each split\n",
    "        fold_metrics: Dict with validation F1 scores for each split\n",
    "        best_scores: Dict with best F1 score for each split plus mean and std\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise containers for results across all splits\n",
    "    fold_losses = {}\n",
    "    fold_metrics = {}\n",
    "    best_scores = {}\n",
    "\n",
    "    # Get model architecture parameters\n",
    "    in_features = len(df.columns[2:-1])\n",
    "    num_classes = len(df['label'].unique())\n",
    "\n",
    "    # Initialise model architecture\n",
    "    model = RecurrentClassifier(\n",
    "        input_size=in_features,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=hidden_layers,\n",
    "        num_classes=num_classes,\n",
    "        dropout_rate=dropout_rate,\n",
    "        bidirectional=bidirectional,\n",
    "        rnn_type=rnn_type\n",
    "    ).to(device)\n",
    "\n",
    "    # Store initial weights to reset model for each split\n",
    "    initial_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # Iterate through K random splits\n",
    "    for split_idx in range(k):\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(f\"Split {split_idx+1}/{k}\")\n",
    "\n",
    "        # Get unique user IDs and shuffle them with split-specific seed\n",
    "        unique_users = df['sample_index'].unique()\n",
    "        random.seed(seed + split_idx)\n",
    "        random.shuffle(unique_users)\n",
    "\n",
    "        # Calculate the number of users for the training set\n",
    "        n_train_users = len(unique_users) - n_val_users\n",
    "\n",
    "        # Split the shuffled user IDs into training, validation, and test sets\n",
    "        train_users = unique_users[:n_train_users]\n",
    "        val_users = unique_users[n_train_users:n_train_users + n_val_users]\n",
    "\n",
    "        # Split the dataset into training, validation, and test sets based on user IDs\n",
    "        df_train = df[df['sample_index'].isin(train_users)].copy()\n",
    "        df_val = df[df['sample_index'].isin(val_users)].copy()\n",
    "\n",
    "        # Map activity names to integers in the training set\n",
    "        df_train['label'] = df_train['label'].map(label_mapping)\n",
    "\n",
    "        # Map activity names to integers in the validation set\n",
    "        df_val['label'] = df_val['label'].map(label_mapping)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(f\"  Training set shape: {df_train.shape}\")\n",
    "            print(f\"  Validation set shape: {df_val.shape}\")\n",
    "\n",
    "        scale_columns = df_train.columns [2:-1]\n",
    "\n",
    "        # Calculate the minimum and maximum values from the training data only\n",
    "        train_max = df_train[scale_columns].min()\n",
    "        train_min = df_train[scale_columns].max()\n",
    "\n",
    "        # Apply normalisation to the specified columns in all datasets\n",
    "        for column in scale_columns:\n",
    "            df_train[column] = (df_train[column] - train_min[column]) / (train_max[column] - train_min[column] + 1e-8)\n",
    "            df_val[column] = (df_val[column] - train_min[column]) / (train_max[column] - train_min[column] + 1e-8)\n",
    "            df_test[column] = (df_test[column] - train_min[column]) / (train_max[column] - train_min[column] + 1e-8)\n",
    "\n",
    "\n",
    "        # Build sequences using the existing build_sequences function\n",
    "        X_train, y_train = build_sequences(df_train, window=window_size, stride=stride)\n",
    "        X_val, y_val = build_sequences(df_val, window=window_size, stride=stride)\n",
    "        X_test, sample_indices = build_sequences_test(df_test, window_size, stride)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(f\"  Training sequences shape: {X_train.shape}\")\n",
    "            print(f\"  Validation sequences shape: {X_val.shape}\")\n",
    "            print(f\"  Test sequences shape: {X_test.shape}\")\n",
    "\n",
    "        # Create PyTorch datasets\n",
    "        train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "        val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "        test_ds = TensorDataset(torch.from_numpy(X_test))\n",
    "\n",
    "        # Create data loaders\n",
    "        train_loader = make_loader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "        val_loader   = make_loader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "        test_loader = make_loader(test_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "\n",
    "        # Reset model to initial weights for fair comparison across splits\n",
    "        model.load_state_dict(initial_state)\n",
    "\n",
    "        # Define optimizer with L2 regularization\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
    "\n",
    "        # Enable mixed precision training for GPU acceleration\n",
    "        split_scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "        # Create directory for model checkpoints\n",
    "        os.makedirs(f\"models/{experiment_name}\", exist_ok=True)\n",
    "\n",
    "        # Train model on current split\n",
    "        model, training_history = fit(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            epochs=epochs,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scaler=split_scaler,\n",
    "            device=device,\n",
    "            writer=writer,\n",
    "            patience=patience,\n",
    "            verbose=verbose,\n",
    "            l1_lambda=l1_lambda,\n",
    "            evaluation_metric=evaluation_metric,\n",
    "            mode=mode,\n",
    "            restore_best_weights=restore_best_weights,\n",
    "            experiment_name=experiment_name+\"/split_\"+str(split_idx)\n",
    "        )\n",
    "\n",
    "        # Store results for this split\n",
    "        fold_losses[f\"split_{split_idx}\"] = training_history['val_loss']\n",
    "        fold_metrics[f\"split_{split_idx}\"] = training_history['val_f1']\n",
    "        best_scores[f\"split_{split_idx}\"] = max(training_history['val_f1'])\n",
    "\n",
    "        #plot_val_loss_and_f1(training_history)\n",
    "        #val_confusion_matrix(val_loader)\n",
    "        df_test_results = last_logits_test(test_loader)\n",
    "        print(df_test_results['labels'].value_counts())\n",
    "        df_test_results.to_csv(\"submission_\"+experiment_name+\"_split_\"+str(split_idx), index=False)\n",
    "\n",
    "    # Compute mean and standard deviation of best scores across splits\n",
    "    best_scores[\"mean\"] = np.mean([best_scores[k] for k in best_scores.keys() if k.startswith(\"split_\")])\n",
    "    best_scores[\"std\"] = np.std([best_scores[k] for k in best_scores.keys() if k.startswith(\"split_\")])\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(f\"Best score: {best_scores['mean']:.4f}±{best_scores['std']:.4f}\")\n",
    "\n",
    "    return fold_losses, fold_metrics, best_scores"
   ],
   "id": "ace3251f8c8d596a",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T13:45:05.954576Z",
     "start_time": "2025-11-08T13:45:05.860709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%time\n",
    "fold_losses, fold_metrics, best_scores = k_shuffle_split_cross_validation_round_rnn(\n",
    "                                            df, EPOCHS, criterion, device,\n",
    "                                            K, N_VAL_USERS, BATCH_SIZE, HIDDEN_LAYERS, HIDDEN_SIZE, LEARNING_RATE, DROPOUT_RATE,\n",
    "                                            WINDOW_SIZE, STRIDE, RNN_TYPE, BIDIRECTIONAL,\n",
    "                                            L1_LAMBDA, L2_LAMBDA, PATIENCE, experiment_name=EXPERIMENT_NAME)"
   ],
   "id": "53e8dd7278b8eaf8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 μs, sys: 1 μs, total: 3 μs\n",
      "Wall time: 4.29 μs\n",
      "Split 1/5\n",
      "  Training set shape: (89760, 40)\n",
      "  Validation set shape: (16000, 40)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'n_legs'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/ml/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001B[39m, in \u001B[36mIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   3811\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m3812\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3813\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/index.pyx:167\u001B[39m, in \u001B[36mpandas._libs.index.IndexEngine.get_loc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/index.pyx:196\u001B[39m, in \u001B[36mpandas._libs.index.IndexEngine.get_loc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001B[39m, in \u001B[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001B[39m, in \u001B[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[31mKeyError\u001B[39m: 'n_legs'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[89]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m get_ipython().run_line_magic(\u001B[33m'\u001B[39m\u001B[33mtime\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m fold_losses, fold_metrics, best_scores = \u001B[43mk_shuffle_split_cross_validation_round_rnn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m                                            \u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mEPOCHS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m                                            \u001B[49m\u001B[43mK\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mN_VAL_USERS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mBATCH_SIZE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mHIDDEN_LAYERS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mHIDDEN_SIZE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mLEARNING_RATE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mDROPOUT_RATE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m                                            \u001B[49m\u001B[43mWINDOW_SIZE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mSTRIDE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mRNN_TYPE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mBIDIRECTIONAL\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m                                            \u001B[49m\u001B[43mL1_LAMBDA\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mL2_LAMBDA\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mPATIENCE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexperiment_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mEXPERIMENT_NAME\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[88]\u001B[39m\u001B[32m, line 108\u001B[39m, in \u001B[36mk_shuffle_split_cross_validation_round_rnn\u001B[39m\u001B[34m(df, epochs, criterion, device, k, n_val_users, batch_size, hidden_layers, hidden_size, learning_rate, dropout_rate, window_size, stride, rnn_type, bidirectional, l1_lambda, l2_lambda, patience, evaluation_metric, mode, restore_best_weights, writer, verbose, seed, experiment_name)\u001B[39m\n\u001B[32m    106\u001B[39m     df_train[column] = (df_train[column] - train_min[column]) / (train_max[column] - train_min[column] + \u001B[32m1e-8\u001B[39m)\n\u001B[32m    107\u001B[39m     df_val[column] = (df_val[column] - train_min[column]) / (train_max[column] - train_min[column] + \u001B[32m1e-8\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m108\u001B[39m     df_test[column] = (\u001B[43mdf_test\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcolumn\u001B[49m\u001B[43m]\u001B[49m - train_min[column]) / (train_max[column] - train_min[column] + \u001B[32m1e-8\u001B[39m)\n\u001B[32m    111\u001B[39m \u001B[38;5;66;03m# Build sequences using the existing build_sequences function\u001B[39;00m\n\u001B[32m    112\u001B[39m X_train, y_train = build_sequences(df_train, window=window_size, stride=stride)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/ml/lib/python3.13/site-packages/pandas/core/frame.py:4113\u001B[39m, in \u001B[36mDataFrame.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   4111\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.columns.nlevels > \u001B[32m1\u001B[39m:\n\u001B[32m   4112\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._getitem_multilevel(key)\n\u001B[32m-> \u001B[39m\u001B[32m4113\u001B[39m indexer = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4114\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[32m   4115\u001B[39m     indexer = [indexer]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/ml/lib/python3.13/site-packages/pandas/core/indexes/base.py:3819\u001B[39m, in \u001B[36mIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   3814\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m   3815\u001B[39m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc.Iterable)\n\u001B[32m   3816\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[32m   3817\u001B[39m     ):\n\u001B[32m   3818\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[32m-> \u001B[39m\u001B[32m3819\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01merr\u001B[39;00m\n\u001B[32m   3820\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[32m   3821\u001B[39m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[32m   3822\u001B[39m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[32m   3823\u001B[39m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[32m   3824\u001B[39m     \u001B[38;5;28mself\u001B[39m._check_indexing_error(key)\n",
      "\u001B[31mKeyError\u001B[39m: 'n_legs'"
     ]
    }
   ],
   "execution_count": 89
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
